---
phase: 04-knowledge-extraction-hooks
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - get-shit-done/bin/knowledge-extraction.js
autonomous: true

must_haves:
  truths:
    - "Decisions detected via regex patterns (let's use, decided to, going with)"
    - "Lessons detected via regex patterns (turns out, I learned, the trick is)"
    - "Quality gates filter short (<20 chars) and non-technical content"
    - "Extraction returns structured objects with type, content, pattern, full_match"
  artifacts:
    - path: "get-shit-done/bin/knowledge-extraction.js"
      provides: "Pattern-based knowledge extraction"
      exports: ["extractFromResponse", "passesQualityGate", "filterWithQualityGates", "DECISION_PATTERNS", "LESSON_PATTERNS"]
  key_links:
    - from: "get-shit-done/bin/knowledge-extraction.js"
      to: "DECISION_PATTERNS"
      via: "regex matchAll"
      pattern: "DECISION_PATTERNS.*matchAll"
    - from: "get-shit-done/bin/knowledge-extraction.js"
      to: "passesQualityGate"
      via: "filter function"
      pattern: "passesQualityGate.*content"
---

<objective>
Create regex-based extraction for decisions and lessons with quality gates for noise prevention

Purpose: Enable passive knowledge capture from Claude responses (HOOK-01, HOOK-02, HOOK-03, HOOK-04, KNOW-12)
Output: knowledge-extraction.js with pattern matching and quality filtering
</objective>

<execution_context>
@/Users/ollorin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ollorin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-knowledge-extraction-hooks/04-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create decision and lesson regex patterns</name>
  <files>get-shit-done/bin/knowledge-extraction.js</files>
  <action>
Create `get-shit-done/bin/knowledge-extraction.js` with pattern definitions:

**Decision patterns (HOOK-02):**
```javascript
const DECISION_PATTERNS = [
  // "let's use X", "let us go with Y"
  /(?:let's|let us)\s+(?:use|go with|implement|choose|try|pick)\s+([^.!?]+)/gi,

  // "decided to X", "decided on Y"
  /(?:decided|decided to|choosing to|going with|chose to)\s+([^.!?]+)/gi,

  // "will use X because/for/to"
  /(?:will use|using|opted for|opting for)\s+([^.!?]+?)\s+(?:because|for|to|since)/gi,

  // "approach: X", "solution: Y"
  /(?:approach|solution|implementation|strategy|plan):\s*([^.!?\n]+)/gi,

  // "I recommend X", "I suggest Y"
  /(?:I recommend|I suggest|I'd suggest|I'd recommend)\s+([^.!?]+)/gi,

  // "X is better because", "X makes more sense"
  /([A-Z][a-z]+(?:\s+[a-zA-Z]+)*)\s+(?:is better|makes more sense|is the way to go|is preferred)/gi
];
```

**Lesson patterns (HOOK-03):**
```javascript
const LESSON_PATTERNS = [
  // "learned that X", "discovered that Y"
  /(?:learned|discovered|found out|realized)\s+(?:that\s+)?([^.!?]+)/gi,

  // "turns out X", "it turns out Y"
  /(?:turns out|it turns out|apparently)\s+(?:that\s+)?([^.!?]+)/gi,

  // "the trick is X", "the key is Y"
  /(?:the trick is|the key is|the secret is|the solution is)\s+([^.!?]+)/gi,

  // "gotcha: X", "pitfall: Y", "watch out: Z"
  /(?:gotcha|pitfall|caveat|watch out|warning|caution):\s*([^.!?\n]+)/gi,

  // "note: X", "important: Y"
  /(?:note|important|remember|tip|hint):\s*([^.!?\n]+)/gi,

  // "X doesn't work because", "X fails when"
  /([^.!?]+?)\s+(?:doesn't work|won't work|fails|breaks)\s+(?:because|when|if)\s+([^.!?]+)/gi,

  // "instead of X, use Y"
  /instead of\s+([^,]+),\s+(?:use|try|do)\s+([^.!?]+)/gi
];
```

**Main extraction function:**
```javascript
function extractFromResponse(responseText) {
  if (!responseText || typeof responseText !== 'string') {
    return [];
  }

  const matches = [];

  // Extract decisions
  for (const pattern of DECISION_PATTERNS) {
    // Reset lastIndex for global regex
    pattern.lastIndex = 0;
    const found = [...responseText.matchAll(pattern)];

    for (const match of found) {
      const content = (match[1] || match[0]).trim();
      if (content.length > 0) {
        matches.push({
          type: 'decision',
          content,
          pattern: pattern.source,
          full_match: match[0].trim(),
          index: match.index
        });
      }
    }
  }

  // Extract lessons
  for (const pattern of LESSON_PATTERNS) {
    pattern.lastIndex = 0;
    const found = [...responseText.matchAll(pattern)];

    for (const match of found) {
      // Handle patterns with multiple capture groups
      const content = (match[2] || match[1] || match[0]).trim();
      if (content.length > 0) {
        matches.push({
          type: 'lesson',
          content,
          pattern: pattern.source,
          full_match: match[0].trim(),
          index: match.index
        });
      }
    }
  }

  // Sort by position in text
  matches.sort((a, b) => a.index - b.index);

  return matches;
}
```

**Exports:**
- DECISION_PATTERNS
- LESSON_PATTERNS
- extractFromResponse(text) -> Match[]
  </action>
  <verify>
```bash
cd get-shit-done && node -e "
const { extractFromResponse, DECISION_PATTERNS, LESSON_PATTERNS } = require('./bin/knowledge-extraction.js');

const testText = \`
Let's use better-sqlite3 for the database layer.
I decided to go with Sonnet for complex tasks.
Turns out that SQLite needs AUTOINCREMENT for reliable rowids.
The trick is to normalize embeddings before storage.
Note: Always use transactions for multi-table operations.
\`;

const matches = extractFromResponse(testText);
console.log('Total matches:', matches.length);
console.log('Decisions:', matches.filter(m => m.type === 'decision').length);
console.log('Lessons:', matches.filter(m => m.type === 'lesson').length);
matches.forEach(m => console.log('-', m.type, ':', m.content.slice(0, 50)));
"
```
  </verify>
  <done>
- Decisions extracted from "let's use", "decided to", etc.
- Lessons extracted from "turns out", "the trick is", etc.
- Each match includes type, content, pattern, full_match, index
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement quality gates for noise prevention</name>
  <files>get-shit-done/bin/knowledge-extraction.js</files>
  <action>
Add quality gates to knowledge-extraction.js (HOOK-04):

**Technical signal detection:**
```javascript
const TECHNICAL_SIGNALS = [
  /`[^`]+`/,                    // Backticks (code references)
  /\/[a-zA-Z0-9_\-./]+/,        // Paths (slashes)
  /\berror\b/i,                 // Error mentions
  /\b(?:npm|git|node|bash|python|rust|javascript|typescript|react|vue|angular)\b/i,
  /\b(?:API|HTTP|JSON|SQL|CLI|URL|HTML|CSS)\b/,
  /\b(?:function|class|const|let|var|import|export|require|module)\b/,
  /\b(?:async|await|promise|callback)\b/i,
  /\b(?:database|schema|model|table|query)\b/i,
  /\b(?:file|directory|path|folder)\b/i,
  /\.[a-z]{2,4}\b/,             // File extensions (.js, .ts, .md)
  /[A-Z][a-z]+[A-Z]/,           // CamelCase identifiers
  /[a-z]+_[a-z]+/               // snake_case identifiers
];

const GENERIC_PHRASES = [
  'sounds good', 'looks good', 'that works', 'makes sense',
  'got it', 'understood', 'okay', 'alright', 'sure',
  'yes', 'no', 'maybe', 'I think so', 'I agree',
  'thank you', 'thanks', 'great', 'perfect', 'awesome'
];
```

**Quality gate function:**
```javascript
function passesQualityGate(content) {
  // Minimum length check (HOOK-04: 20 chars)
  if (!content || content.length < 20) {
    return { passed: false, reason: 'too_short', threshold: 20, actual: content?.length || 0 };
  }

  // Avoid generic phrases
  const lowerContent = content.toLowerCase();
  const isGeneric = GENERIC_PHRASES.some(phrase =>
    lowerContent.includes(phrase) && content.length < 50
  );
  if (isGeneric) {
    return { passed: false, reason: 'generic_phrase' };
  }

  // Technical signal detection
  const hasTechnicalSignal = TECHNICAL_SIGNALS.some(regex => regex.test(content));
  if (!hasTechnicalSignal) {
    return { passed: false, reason: 'no_technical_signal' };
  }

  return { passed: true };
}
```

**Filter function:**
```javascript
function filterWithQualityGates(extractions, options = {}) {
  const { debug = false } = options;

  return extractions.filter(ext => {
    const check = passesQualityGate(ext.content);

    if (!check.passed) {
      if (debug || process.env.GSD_DEBUG) {
        console.log(`[extraction] Filtered: "${ext.content.slice(0, 40)}..." - ${check.reason}`);
      }
      return false;
    }

    return true;
  });
}
```

**Combined extraction with filtering:**
```javascript
function extractAndFilter(responseText, options = {}) {
  const raw = extractFromResponse(responseText);
  return filterWithQualityGates(raw, options);
}
```

**Add to exports:**
- TECHNICAL_SIGNALS
- GENERIC_PHRASES
- passesQualityGate(content) -> { passed, reason? }
- filterWithQualityGates(extractions, options?) -> filteredExtractions
- extractAndFilter(text, options?) -> filteredExtractions
  </action>
  <verify>
```bash
cd get-shit-done && node -e "
const { passesQualityGate, filterWithQualityGates, extractAndFilter } = require('./bin/knowledge-extraction.js');

// Test quality gates
console.log('Short text:', passesQualityGate('too short'));
console.log('Generic:', passesQualityGate('sounds good to me, that works fine'));
console.log('No tech:', passesQualityGate('I think we should proceed with the plan'));
console.log('Valid:', passesQualityGate('Use better-sqlite3 for the database layer'));

// Test combined extraction
const text = \`
Let's use SQLite for storage.
Sounds good.
The trick is to use AUTOINCREMENT for reliable IDs.
Okay.
\`;

const results = extractAndFilter(text, { debug: true });
console.log('\\nFiltered results:', results.length);
results.forEach(r => console.log('-', r.type, ':', r.content));
"
```
  </verify>
  <done>
- passesQualityGate returns { passed: true } for technical content >= 20 chars
- passesQualityGate returns { passed: false, reason } for filtered content
- filterWithQualityGates removes low-quality extractions
- extractAndFilter combines extraction + filtering
  </done>
</task>

<task type="auto">
  <name>Task 3: Add deduplication within extraction batch</name>
  <files>get-shit-done/bin/knowledge-extraction.js</files>
  <action>
Add within-batch deduplication to prevent duplicate extractions from same response:

**Content hash for exact match detection:**
```javascript
const crypto = require('crypto');

function computeContentHash(content) {
  return crypto.createHash('sha256').update(content.trim()).digest('hex');
}

function computeCanonicalHash(content) {
  const canonical = content
    .toLowerCase()
    .replace(/\s+/g, ' ')
    .replace(/[.,;:!?'"]/g, '')
    .trim();
  return crypto.createHash('sha256').update(canonical).digest('hex');
}
```

**Deduplicate within batch:**
```javascript
function deduplicateExtractions(extractions) {
  const seen = new Map();  // canonical_hash -> extraction
  const deduplicated = [];

  for (const ext of extractions) {
    const canonical = computeCanonicalHash(ext.content);

    if (!seen.has(canonical)) {
      seen.set(canonical, ext);
      deduplicated.push({
        ...ext,
        content_hash: computeContentHash(ext.content),
        canonical_hash: canonical
      });
    }
  }

  return deduplicated;
}
```

**Full extraction pipeline:**
```javascript
function extractKnowledge(responseText, options = {}) {
  // Step 1: Extract raw matches
  const raw = extractFromResponse(responseText);

  // Step 2: Apply quality gates
  const filtered = filterWithQualityGates(raw, options);

  // Step 3: Deduplicate within batch
  const deduplicated = deduplicateExtractions(filtered);

  return {
    total_raw: raw.length,
    total_filtered: filtered.length,
    total_deduplicated: deduplicated.length,
    extractions: deduplicated
  };
}
```

**Add to exports:**
- computeContentHash(content) -> string
- computeCanonicalHash(content) -> string
- deduplicateExtractions(extractions) -> deduplicatedExtractions
- extractKnowledge(text, options?) -> { total_raw, total_filtered, total_deduplicated, extractions }
  </action>
  <verify>
```bash
cd get-shit-done && node -e "
const { extractKnowledge, computeContentHash, computeCanonicalHash } = require('./bin/knowledge-extraction.js');

// Test hash functions
console.log('Content hash:', computeContentHash('test').slice(0, 16));
console.log('Canonical same:', computeCanonicalHash('Use SQLite') === computeCanonicalHash('use sqlite'));

// Test full pipeline with duplicates
const text = \`
Let's use SQLite for storage.
I decided to use SQLite for the database.
The trick is to use AUTOINCREMENT for reliable IDs.
\`;

const result = extractKnowledge(text);
console.log('\\nPipeline results:');
console.log('- Raw:', result.total_raw);
console.log('- Filtered:', result.total_filtered);
console.log('- Deduplicated:', result.total_deduplicated);
result.extractions.forEach(e => console.log('-', e.type, ':', e.content.slice(0, 40)));
"
```
  </verify>
  <done>
- extractKnowledge runs full pipeline (extract -> filter -> dedupe)
- Duplicate content with different casing/punctuation collapsed
- Each extraction has content_hash and canonical_hash
- Pipeline stats returned (total_raw, total_filtered, total_deduplicated)
  </done>
</task>

</tasks>

<verification>
All extraction tests pass:
- DECISION_PATTERNS match "let's use", "decided to", "going with", etc.
- LESSON_PATTERNS match "turns out", "the trick is", "gotcha:", etc.
- Quality gates filter short, generic, and non-technical content
- Within-batch deduplication removes near-duplicate extractions
- extractKnowledge provides full pipeline with stats
</verification>

<success_criteria>
1. extractFromResponse detects decisions and lessons from text
2. passesQualityGate filters content < 20 chars
3. passesQualityGate requires technical signals (backticks, paths, keywords)
4. filterWithQualityGates removes generic phrases
5. deduplicateExtractions collapses near-duplicates via canonical hash
6. extractKnowledge combines all steps with pipeline stats
</success_criteria>

<output>
After completion, create `.planning/phases/04-knowledge-extraction-hooks/04-02-SUMMARY.md`
</output>
