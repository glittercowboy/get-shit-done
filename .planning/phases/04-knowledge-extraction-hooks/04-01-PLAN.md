---
phase: 04-knowledge-extraction-hooks
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - get-shit-done/bin/embeddings.js
autonomous: true

must_haves:
  truths:
    - "Local embeddings can be generated without API calls"
    - "Embeddings are 512 dimensions (Matryoshka)"
    - "Model loads lazily on first use, not at import time"
    - "Graceful degradation when model unavailable"
  artifacts:
    - path: "get-shit-done/bin/embeddings.js"
      provides: "Local embedding generation"
      exports: ["generateEmbedding", "initEmbeddings", "isEmbeddingsAvailable"]
  key_links:
    - from: "get-shit-done/bin/embeddings.js"
      to: "@xenova/transformers"
      via: "pipeline import"
      pattern: "require.*@xenova/transformers"
---

<objective>
Create local embedding generation using transformers.js with Nomic Embed model for offline semantic similarity

Purpose: Enable semantic deduplication (KNOW-16 stage 3) and synthesis (KNOW-15) without API dependencies or costs
Output: embeddings.js module with lazy-loading, caching, and graceful degradation
</objective>

<execution_context>
@/Users/ollorin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ollorin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-knowledge-extraction-hooks/04-RESEARCH.md

# Phase 3 knowledge modules (for integration patterns)
@get-shit-done/bin/knowledge.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install transformers.js and create embeddings module</name>
  <files>get-shit-done/bin/embeddings.js, package.json</files>
  <action>
1. Install @xenova/transformers:
   ```bash
   cd get-shit-done && npm install @xenova/transformers
   ```

2. Create `get-shit-done/bin/embeddings.js` with:

**Configuration:**
- Model: nomic-ai/nomic-embed-text-v1.5 (512-dim Matryoshka)
- Use quantized version for smaller size
- Configure for local caching (allowLocalModels: true, allowRemoteModels: true)

**Lazy loading pattern:**
```javascript
let embeddingPipeline = null;

async function initEmbeddings() {
  if (embeddingPipeline) return embeddingPipeline;

  const { pipeline, env } = require('@xenova/transformers');
  env.allowLocalModels = true;
  env.allowRemoteModels = true;

  embeddingPipeline = await pipeline(
    'feature-extraction',
    'nomic-ai/nomic-embed-text-v1.5',
    { quantized: true }
  );

  return embeddingPipeline;
}
```

**Main embedding function:**
```javascript
async function generateEmbedding(text) {
  try {
    const pipe = await initEmbeddings();
    const output = await pipe(text, {
      pooling: 'mean',
      normalize: true  // L2 normalization
    });

    // Extract as Float32Array, truncate to 512 dims
    let embedding = output.data;
    if (embedding.length > 512) {
      embedding = new Float32Array(embedding.slice(0, 512));
    }

    return embedding;
  } catch (err) {
    console.warn('Embedding generation failed:', err.message);
    return null;  // Graceful degradation
  }
}
```

**Availability check:**
```javascript
async function isEmbeddingsAvailable() {
  try {
    await initEmbeddings();
    return { available: true };
  } catch (err) {
    return { available: false, reason: err.message };
  }
}
```

**Exports:**
- generateEmbedding(text) -> Float32Array(512) | null
- initEmbeddings() -> pipeline | throws
- isEmbeddingsAvailable() -> { available, reason? }

3. Do NOT use any other embedding library. Do NOT call external APIs. This must be fully local.
  </action>
  <verify>
```bash
cd get-shit-done && node -e "
const { generateEmbedding, isEmbeddingsAvailable } = require('./bin/embeddings.js');
(async () => {
  const status = await isEmbeddingsAvailable();
  console.log('Available:', status);
  if (status.available) {
    const emb = await generateEmbedding('test embedding');
    console.log('Embedding length:', emb?.length);
    console.log('Type:', emb?.constructor?.name);
  }
})();
"
```
  </verify>
  <done>
- generateEmbedding('text') returns Float32Array(512)
- isEmbeddingsAvailable() returns { available: true } after model download
- No API calls made (fully local)
  </done>
</task>

<task type="auto">
  <name>Task 2: Add batch embedding and caching</name>
  <files>get-shit-done/bin/embeddings.js</files>
  <action>
Extend embeddings.js with batch processing for efficiency:

**Batch embedding function:**
```javascript
async function generateEmbeddingBatch(texts) {
  if (!texts || texts.length === 0) return [];

  try {
    const pipe = await initEmbeddings();
    const results = [];

    // Process in batches of 10 to avoid memory issues
    const batchSize = 10;
    for (let i = 0; i < texts.length; i += batchSize) {
      const batch = texts.slice(i, i + batchSize);

      for (const text of batch) {
        const output = await pipe(text, {
          pooling: 'mean',
          normalize: true
        });

        let embedding = output.data;
        if (embedding.length > 512) {
          embedding = new Float32Array(embedding.slice(0, 512));
        }
        results.push(embedding);
      }
    }

    return results;
  } catch (err) {
    console.warn('Batch embedding failed:', err.message);
    return texts.map(() => null);
  }
}
```

**Simple in-memory cache for repeated queries:**
```javascript
const embeddingCache = new Map();
const CACHE_MAX_SIZE = 1000;

async function generateEmbeddingCached(text) {
  if (embeddingCache.has(text)) {
    return embeddingCache.get(text);
  }

  const embedding = await generateEmbedding(text);

  if (embedding && embeddingCache.size < CACHE_MAX_SIZE) {
    embeddingCache.set(text, embedding);
  }

  return embedding;
}

function clearEmbeddingCache() {
  embeddingCache.clear();
}
```

**Add to exports:**
- generateEmbeddingBatch(texts) -> Float32Array[]
- generateEmbeddingCached(text) -> Float32Array | null
- clearEmbeddingCache() -> void
  </action>
  <verify>
```bash
cd get-shit-done && node -e "
const { generateEmbeddingBatch, generateEmbeddingCached, clearEmbeddingCache } = require('./bin/embeddings.js');
(async () => {
  // Test batch
  const batch = await generateEmbeddingBatch(['hello', 'world']);
  console.log('Batch count:', batch.length);
  console.log('Each length:', batch.map(e => e?.length));

  // Test cache
  const emb1 = await generateEmbeddingCached('cached test');
  const emb2 = await generateEmbeddingCached('cached test');
  console.log('Cache hit (same ref):', emb1 === emb2);

  clearEmbeddingCache();
  console.log('Cache cleared');
})();
"
```
  </verify>
  <done>
- Batch embedding processes multiple texts efficiently
- Cache prevents re-computing same embedding
- clearEmbeddingCache() resets cache
  </done>
</task>

</tasks>

<verification>
All embeddings tests pass:
- Single embedding generates 512-dim Float32Array
- Batch embedding processes multiple texts
- Cache prevents duplicate computation
- Graceful null return on errors (no throws)
- Model loads on first use only (lazy)
</verification>

<success_criteria>
1. `generateEmbedding('text')` returns 512-dim Float32Array
2. `generateEmbeddingBatch(['a', 'b', 'c'])` returns array of embeddings
3. `generateEmbeddingCached(text)` returns cached value on repeat
4. `isEmbeddingsAvailable()` returns availability status
5. No external API calls (fully offline after model download)
6. Graceful degradation on errors (returns null, not throws)
</success_criteria>

<output>
After completion, create `.planning/phases/04-knowledge-extraction-hooks/04-01-SUMMARY.md`
</output>
