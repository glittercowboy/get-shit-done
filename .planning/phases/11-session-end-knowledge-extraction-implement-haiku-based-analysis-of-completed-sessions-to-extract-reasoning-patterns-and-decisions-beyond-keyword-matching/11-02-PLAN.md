---
phase: 11-session-end-knowledge-extraction
plan: 02
type: execute
wave: 2
depends_on: ["11-01"]
files_modified:
  - get-shit-done/bin/session-quality-gates.js
  - get-shit-done/bin/session-chunker.js
autonomous: true

must_haves:
  truths:
    - "Sessions with fewer than 2 questions, 2 answers, or 10 total entries are skipped (not analyzed)"
    - "Already-analyzed sessions are detected via content hash and not re-analyzed"
    - "Sessions exceeding 25k characters are split into entry-boundary chunks with session metadata preserved in each chunk"
    - "Voice message entries are automatically resolved to their transcribed text before analysis"
  artifacts:
    - path: "get-shit-done/bin/session-quality-gates.js"
      provides: "Session filtering (shouldAnalyze), content hashing (getSessionHash), analysis tracking (markAnalyzed, isAlreadyAnalyzed)"
      contains: "shouldAnalyzeSession"
    - path: "get-shit-done/bin/session-chunker.js"
      provides: "Session chunking for large transcripts, voice-to-text resolution"
      contains: "chunkSession"
  key_links:
    - from: "get-shit-done/bin/session-quality-gates.js"
      to: "get-shit-done/bin/session-analyzer.js"
      via: "Called before analyzeSession to gate analysis"
      pattern: "shouldAnalyzeSession"
    - from: "get-shit-done/bin/session-chunker.js"
      to: "get-shit-done/bin/analysis-prompts.js"
      via: "formatEntriesForPrompt used to measure chunk size"
      pattern: "formatEntriesForPrompt"
---

<objective>
Implement session quality gates to prevent wasteful analysis of trivial sessions, content hashing for re-analysis prevention, and session chunking for large transcripts that exceed Haiku's effective window.

Purpose: Cost control and quality assurance. Without quality gates, every 3-entry test session triggers Haiku analysis. Without chunking, sessions >30k chars lose context or fail. Without re-analysis prevention, the same session gets analyzed repeatedly.

Output: session-quality-gates.js (filtering + hash tracking), session-chunker.js (chunking + voice text resolution)
</objective>

<execution_context>
@/Users/ollorin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ollorin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-session-end-knowledge-extraction-implement-haiku-based-analysis-of-completed-sessions-to-extract-reasoning-patterns-and-decisions-beyond-keyword-matching/11-CONTEXT.md
@.planning/phases/11-session-end-knowledge-extraction-implement-haiku-based-analysis-of-completed-sessions-to-extract-reasoning-patterns-and-decisions-beyond-keyword-matching/11-RESEARCH.md
@.planning/phases/11-session-end-knowledge-extraction-implement-haiku-based-analysis-of-completed-sessions-to-extract-reasoning-patterns-and-decisions-beyond-keyword-matching/11-01-SUMMARY.md
@get-shit-done/bin/analysis-prompts.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create session quality gates and re-analysis prevention</name>
  <files>
    get-shit-done/bin/session-quality-gates.js
  </files>
  <action>
Create session-quality-gates.js with the following exports (CommonJS pattern):

**`shouldAnalyzeSession(entries)`** - Returns `{analyze: boolean, reason: string}`:
- Count entries by type: questions, answers, user_messages, bot_responses
- Minimum thresholds: questionCount >= 2 AND answerCount >= 2 AND totalEntries >= 10
- Skip sessions that are all heartbeats/metadata (no substantive interaction)
- Return descriptive reason when skipping (e.g., "Only 1 question (minimum 2)")

**`getSessionContentHash(entries)`** - Returns SHA-256 hex hash of session content:
- Filter to substantive entries only (question, answer, user_message, bot_response)
- Sort by timestamp for deterministic ordering
- Concatenate type + content/question/answer fields
- Compute SHA-256 hash of concatenated string
- This hash is used to detect if session content changed since last analysis

**`markSessionAnalyzed(sessionId, contentHash, insightCount)`** - Persists analysis record:
- Store in `.planning/telegram-sessions/.analysis-log.jsonl` (append-only)
- Each line: `{session_id, content_hash, analyzed_at (ISO), insight_count, version: 1}`
- Use append (not rewrite) for crash safety

**`isAlreadyAnalyzed(sessionId, contentHash)`** - Returns boolean:
- Read `.analysis-log.jsonl` and check if any entry matches both session_id AND content_hash
- If session_id exists but hash differs, return false (content changed, re-analyze)
- If no analysis-log exists, return false

**`getAnalysisStats()`** - Returns summary object:
- Total sessions analyzed
- Total insights extracted
- Last analysis timestamp
- Read from .analysis-log.jsonl

Use CommonJS module.exports. Use crypto.createHash('sha256') for hashing (same pattern as knowledge-dedup.js).
  </action>
  <verify>
    node -e "const g = require('./get-shit-done/bin/session-quality-gates.js'); console.log(Object.keys(g)); const r = g.shouldAnalyzeSession([{type:'question'},{type:'answer'}]); console.log(r.analyze === false, r.reason);"
  </verify>
  <done>
    shouldAnalyzeSession correctly rejects sessions below thresholds. getSessionContentHash produces deterministic hashes. markSessionAnalyzed persists to JSONL. isAlreadyAnalyzed detects both session_id and content_hash matches. No external dependencies beyond Node.js crypto.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create session chunker with voice text resolution</name>
  <files>
    get-shit-done/bin/session-chunker.js
  </files>
  <action>
Create session-chunker.js with the following exports (CommonJS pattern):

**`chunkSession(entries, maxCharsPerChunk = 25000)`** - Returns array of entry arrays:
- Use `formatEntriesForPrompt` from analysis-prompts.js to measure character count of formatted text
- Split entries at entry boundaries (never mid-entry) to stay under maxCharsPerChunk
- Each chunk preserves the session_metadata entry (first entry) as context header
- If session fits in one chunk, return `[entries]` (single chunk array)
- Algorithm: iterate entries, accumulate formatted size, start new chunk when exceeding limit
- Return `{chunks: SessionEntry[][], totalChars: number, chunkCount: number}`

**`resolveVoiceEntries(entries)`** - Returns entries with voice messages resolved to text:
- Scan entries for type === 'voice_message' or entries containing audio file references
- For entries that have both `voice_file` and `transcription` fields, replace content with transcription text
- For entries that have `voice_file` but no `transcription`, mark with `[Voice message - transcription unavailable]`
- This is a pre-processing step before analysis (Whisper transcription happens at message receipt time per Phase 8, this just ensures the text is used for analysis)
- Return new array (do not mutate input)

**`prepareSessionForAnalysis(entries)`** - Convenience function combining both:
1. Call `resolveVoiceEntries(entries)` to get text-only entries
2. Call `chunkSession(resolvedEntries)` to get chunks
3. Return `{chunks, resolvedEntries, totalChars, chunkCount}`

Use CommonJS. Require analysis-prompts.js for formatEntriesForPrompt.
  </action>
  <verify>
    node -e "const c = require('./get-shit-done/bin/session-chunker.js'); console.log(Object.keys(c)); const r = c.chunkSession([{type:'session_metadata'},{type:'question',question:'test',timestamp:'2026-01-01'}]); console.log(r.chunkCount === 1);"
  </verify>
  <done>
    chunkSession splits large sessions at entry boundaries respecting 25k char limit. Each chunk preserves session metadata. resolveVoiceEntries converts voice entries to text. prepareSessionForAnalysis combines both steps. No chunking occurs for sessions under the limit.
  </done>
</task>

</tasks>

<verification>
1. `node -e "require('./get-shit-done/bin/session-quality-gates.js')"` loads without error
2. `node -e "require('./get-shit-done/bin/session-chunker.js')"` loads without error
3. Quality gates reject session with 1 question: `shouldAnalyzeSession([{type:'question'},{type:'answer'}]).analyze === false`
4. Quality gates accept session with sufficient content (2+ questions, 2+ answers, 10+ entries)
5. Content hash is deterministic (same entries produce same hash)
6. Chunker produces single chunk for small sessions
7. No imports of `@anthropic-ai/sdk` in any new files
</verification>

<success_criteria>
- Quality gates prevent analysis of trivial sessions (< 2 questions or < 2 answers or < 10 entries)
- Content hash tracking prevents re-analyzing unchanged sessions
- Sessions >25k chars are split at entry boundaries into manageable chunks
- Voice message entries resolved to their text transcriptions before analysis
</success_criteria>

<output>
After completion, create `.planning/phases/11-session-end-knowledge-extraction-implement-haiku-based-analysis-of-completed-sessions-to-extract-reasoning-patterns-and-decisions-beyond-keyword-matching/11-02-SUMMARY.md`
</output>
